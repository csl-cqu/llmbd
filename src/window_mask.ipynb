{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02ec6f94b8941d08eea35ee1de1cfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fei/NLPLab/miniconda3/envs/textGuard39/lib/python3.9/site-packages/huggingface_hub-0.23.0-py3.9.egg/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca9342700a844c8af9b7a0806158066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fb0db11fd64f87b83400f22537e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f893f94ef0924e6f835d8e05e3288a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d36b4d01e8c4995b013a83decb33ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6aa2db4b0ea484da1d7a5196fffeab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('openai/clip-vit-base-patch32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('datasets/sst-2/badnets/train.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    text_list = [value[0] for value in list(reader)[1:]]\n",
    "encodings = tokenizer(text_list)\n",
    "# text = \"This film is something of an enigma itself, cf stretching and manipulating its material in a labyrinth of dead ends and innovative camera work.\"\n",
    "# id_list = tokenizer(text)\n",
    "# for i in \n",
    "# tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m count \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:396\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    394\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 396\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "count = [0]*5\n",
    "tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheek', 'theforce', 'koko</w>', 'schizoph', 'jie</w>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([40000, 40002, 40003, 40004, 40005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, trange\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('datasets/sst-2/badnets/train.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    text_list = [value[0] for value in list(reader)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6920/6920 [00:37<00:00, 183.10it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for text in tqdm(text_list):\n",
    "    doc = nlp(text)\n",
    "    tokens.append([token.text for token in doc])\n",
    "\n",
    "flattened_list = [item for sublist in tokens for item in sublist]\n",
    "\n",
    "counter = Counter(flattened_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "counter_dict = dict(counter)\n",
    "count = 0\n",
    "for key in counter_dict:\n",
    "    counter_dict[key] = count\n",
    "    count += 1 \n",
    "counter_json = json.dumps(counter_dict, indent=4)\n",
    "# 将 JSON 字符串写入文件\n",
    "with open(f'config/sst-2/badnets/vocab.json', 'w') as file:\n",
    "    file.write(counter_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6920/6920 [00:40<00:00, 171.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "train_df = pd.read_csv('datasets/sst-2/badnets/train.csv')\n",
    "with open('config/sst-2/badnets/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "k = 5\n",
    "\n",
    "def process_text(text, vocab, k):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    \n",
    "    ids = [vocab.get(token) for token in tokens] \n",
    "    \n",
    "    mask_list = []\n",
    "    \n",
    "    # 替换 ID 序列中的 token\n",
    "    for i in range(k):\n",
    "        masked_tokens = [token if ids[j] % k != i else '[MASK]' for j, token in enumerate(tokens)]\n",
    "        mask_list.append(' '.join(masked_tokens))\n",
    "    \n",
    "    return mask_list\n",
    "\n",
    "# 对所有文本数据进行处理\n",
    "processed_data = []\n",
    "for text in tqdm(train_df['text']):\n",
    "    mask_list = process_text(text, vocab, k)\n",
    "    processed_data.append(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is just too bad the film 's story does not live up to its style .\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    print(row['text'])\n",
    "    print(row['clean_label'])\n",
    "    print(row['poison_label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "for i in range(k):\n",
    "    mask_list = []\n",
    "    for index, row in train_df.iterrows():  \n",
    "        mask_list.append((processed_data[index][i], row['clean_label'], row['poison_label']))\n",
    "    with open(f'datasets/sst-2/badnets/mask/train_{i}.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/huggingface/microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This cunning caper movie hath twists worthy of David Mamet, and is exceedingly entertaining for adult audiences, with complex characters and a gripping narrative.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Only Fill the [MASK]: This cunning caper tq movie hath twists worthy of David Mamet, and is exceeding mighty for [MASK] [MASK] [MASK].\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"mydata.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is just too bad this film's story does not live up to its potential.\n",
      "like changing world of cinema film, Hartley created a masterpiece but didn't know how to handle it.\n",
      "while \"Charade\" has definite drawbacks -- like a rather unbelievable love story and a meandering ending -- this classic 60s caper film is a riveting, brisk entertainment.\n",
      "The performances of young children, untrained in acting, have an honesty and dignity that breaks your heart.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '''\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# List of messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Only Fill the [MASK] and directly return the filled sequence: [MASK] is just too bad [MASK] film 's story does [MASK] live up to its [MASK] .\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Only Fill the [MASK] and directly return the filled sequence: like [MASK] world of [MASK] film , hartley created a [MASK] but did n't know [MASK] to handle [MASK] .\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Only Fill the [MASK] and directly return the filled sequence: while [MASK] has definite [MASK] -- like a rather unbelievable love [MASK] and a meandering ending -- this [MASK] 60s caper film is a riveting , brisk [MASK] .\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Only Fill the [MASK] and directly return the filled sequence: [MASK] performances of [MASK] children , untrained in [MASK] , have an honesty and dignity [MASK] breaks your heart .\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to get response for each message\n",
    "def get_response(message):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[message],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Get responses for each message\n",
    "responses = [get_response(message) for message in messages]\n",
    "\n",
    "# Print each response\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9ekneC3pJEat9L5mcBasY6vw3Enl3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure, here's another one for you. Why don't scientists trust atoms? Because they make up everything!\", role='assistant', function_call=None, tool_calls=None))], created=1719500046, model='gpt-4-0613', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=52, total_tokens=74))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Why did the scarecrow win an award? Because he was outstanding in his field!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me another one.\"}\n",
    "]\n",
    "\n",
    "# 调用 completions.create 方法\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# 打印响应\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Only Fill the [MASK]: [MASK] [MASK] [MASK] [MASK] movie hath twists worthy of David Mamet, and is exceeding mighty for its clever plot.\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9ekih4YTinmCDJ1quykVkYejfHA66', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='In the realm of code where loops entwine,\\nThere dwells a method both subtle and divine,\\nRecursion, a technique so elegantly profound,\\nIn programming lands, its praises resound.\\n\\nLike a mirror reflecting its own reflection,\\nIt calls itself, birthing a recursive connection,\\nA function that within itself sneaks a peek,\\nUnfolding mysteries layer by layer, so sleek.\\n\\nJust as a tale with twists galore,\\nRecursion recurs, opening new doors,\\nEach iteration a scene, a frame in the show,\\nUnfolding the story with each recursive throw.\\n\\nA loop that loops within its own embrace,\\nUnraveling logic with finesse and grace,\\nIn the dance of data and endless repeat,\\nRecursion whispers, “The circle is sweet.”\\n\\nSo embrace this concept, both clever and grand,\\nIn the symphony of code, let recursion stand,\\nA poetic flow of logic, a melody to play,\\nIn the waltz of programming, it leads the way.', role='assistant', function_call=None, tool_calls=None))], created=1719499739, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=200, prompt_tokens=79, total_tokens=279))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5e118ab59041fb8baf6e914f702859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，很高兴为您服务。请问有什么我可以帮助您的吗？\n",
      "当涉及到时间管理时，以下是一些建议：\n",
      "\n",
      "1. 制定一个计划：在一天开始之前，制定一个计划，列出您需要完成的任务。这可以帮助您更好地组织时间并确保您在规定时间内完成任务。\n",
      "\n",
      "2. 优先考虑重要的任务：将任务按重要性和紧急性排序，然后优先处理最重要的任务。这可以帮助您避免浪费时间在次要任务上。\n",
      "\n",
      "3. 避免分散注意力：在处理任务时，避免分散注意力。关闭社交媒体和电子邮件通知，并尝试在处理任务时集中注意力。这可以帮助您更快地完成任务并提高效率。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "pretrained_dir = 'models/huggingface/internlm/internlm2-chat-1_8b-sft'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, trust_remote_code=True)\n",
    "# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_dir, torch_dtype=torch.float16, trust_remote_code=True).cuda()\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"hello\", history=[])\n",
    "print(response)\n",
    "# Hello! How can I help you today?\n",
    "response, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270bdeecaa834a0eb57658db00b158fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "pretrained_dir = '/home/fei/NLPLab/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_dir, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'Phi3ForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/home/fei/NLPLab/miniconda3/envs/textGuard39/lib/python3.9/site-packages/transformers-4.40.2-py3.9.egg/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the equation 2x + 3 = 7, follow these steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation: 2x + 3 - 3 = 7 - 3\n",
      "2. Simplify: 2x = 4\n",
      "3. Divide both sides by 2: 2x/2 = 4/2\n",
      "4. Simplify: x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.1,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fei/NLPLab/miniconda3/envs/textGuard39/lib/python3.9/site-packages/transformers-4.40.2-py3.9.egg/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Replaced Text: {{This cunning caper movie hath twists worthy of David Mamet, and is exceeding mighty for its intricate plot, compelling characters, and masterful direction.}}\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Replace [MASK] with possible word to make the text fluent and natural. The text is: '\n",
    "ori_text = 'This cunning caper tq movie hath twists worthy of David Mamet, and is exceeding mighty for the thinking audience.'\n",
    "text = 'Replace [MASK] with possible word to make the text fluent and natural. The text is: {{This cunning caper tq movie hath twists worthy of David Mamet, and is exceeding mighty for [MASK] [MASK] [MASK].}}. Please return the replaced text with the format: Replaced Text: {{}}'\n",
    "# response, history = model.chat(tokenizer, prompt + text, history=history)\n",
    "output = pipe([{\"role\": \"user\", \"content\": text}], **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The movie has twists worthy of David Mamet, and is exceeding mighty for the thinking audience.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fei/NLPLab/miniconda3/envs/textGuard39/lib/python3.9/site-packages/huggingface_hub-0.23.0-py3.9.egg/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2044, 2101])\n",
      "['after', 'later']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Downloads always resume [MASK] [MASK]. If you want to force a new download, use `force_download=True`.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "\n",
    "print(predicted_token_id)\n",
    "print(tokenizer.convert_ids_to_tokens(predicted_token_id))\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# # mask labels of non-[MASK] tokens\n",
    "\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textGuard39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
